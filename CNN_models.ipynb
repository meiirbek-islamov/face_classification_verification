{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"jwLEd0gdPbSc","executionInfo":{"status":"ok","timestamp":1649024388761,"user_tz":240,"elapsed":6500,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"21dc944b-1855-4144-cb92-5f20b837f3d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["1.10.0+cu111\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","import torchvision.transforms as ttf\n","\n","import os\n","import os.path as osp\n","\n","from tqdm import tqdm\n","from PIL import Image\n","from sklearn.metrics import roc_auc_score\n","import numpy as np\n","import random\n","from torchsummary import summary\n","\n","print(torch.__version__)"]},{"cell_type":"markdown","source":["# TODOs\n","As you go, please read the code and keep an eye out for TODOs!"],"metadata":{"id":"1oxQNl-YVWHc"}},{"cell_type":"markdown","metadata":{"id":"scOnMklwWBY6"},"source":["# Download Data"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6BksgPdkQwwb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649024391400,"user_tz":240,"elapsed":2654,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}},"outputId":"c8f551c0-470c-44ed-ea70-17d5efec81bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting kaggle==1.5.8\n","  Downloading kaggle-1.5.8.tar.gz (59 kB)\n","\u001b[?25l\r\u001b[K     |█████▌                          | 10 kB 39.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20 kB 44.7 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 30 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 40 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 51 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 59 kB 5.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: kaggle\n","  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73275 sha256=5d9c79adaa5f03c49df5697397d72d2a005930376ba27bdc4c28e91d090b9b27\n","  Stored in directory: /root/.cache/pip/wheels/de/f7/d8/c3902cacb7e62cb611b1ad343d7cc07f42f7eb76ae3a52f3d1\n","Successfully built kaggle\n","Installing collected packages: kaggle\n","  Attempting uninstall: kaggle\n","    Found existing installation: kaggle 1.5.12\n","    Uninstalling kaggle-1.5.12:\n","      Successfully uninstalled kaggle-1.5.12\n","Successfully installed kaggle-1.5.8\n"]}],"source":["!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n","!mkdir /root/.kaggle\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"meiirbekislamov\",\"key\":\"af197071383b4332b004369ebae2a753\"}') # Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3oFjaJTaRjT7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"78bbcbbb-8420-4486-906d-649e5d15999e","executionInfo":{"status":"ok","timestamp":1649024473518,"user_tz":240,"elapsed":82127,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading 11-785-s22-hw2p2-classification.zip to /content\n"," 99% 2.34G/2.35G [00:34<00:00, 66.5MB/s]\n","100% 2.35G/2.35G [00:34<00:00, 72.4MB/s]\n","Downloading 11-785-s22-hw2p2-verification.zip to /content\n"," 95% 249M/263M [00:02<00:00, 108MB/s]\n","100% 263M/263M [00:02<00:00, 119MB/s]\n","11-785-s22-hw2p2-classification.zip   sample_data\n","11-785-s22-hw2p2-verification.zip     train_subset\n","classification\t\t\t      verification\n","classification_sample_submission.csv  verification_sample_submission.csv\n"]}],"source":["!kaggle competitions download -c 11-785-s22-hw2p2-classification\n","!kaggle competitions download -c 11-785-s22-hw2p2-verification\n","\n","!unzip -q 11-785-s22-hw2p2-classification.zip\n","!unzip -q 11-785-s22-hw2p2-verification.zip\n","\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"cBTLCyocZBGS"},"source":["# Hyperparameters for Face Verification"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"13usn4nYZCvJ","executionInfo":{"status":"ok","timestamp":1649024700002,"user_tz":240,"elapsed":534,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"outputs":[],"source":["\"\"\"\n","The well-accepted SGD batch_size & lr combination for CNN classification is 256 batch size for 0.1 learning rate.\n","When changing batch size for SGD, follow the linear scaling rule - halving batch size -> halve learning rate, etc.\n","This is less theoretically supported for Adam, but in my experience, it's a decent ballpark estimate.\n","\"\"\"\n","batch_size = 256\n","lr = 0.1\n","epochs = 50 # Just for the early submission. We'd want you to train like 50 epochs for your main submissions."]},{"cell_type":"markdown","source":["# Hyperparameters for Face Classification"],"metadata":{"id":"pRshdZWgt8IT"}},{"cell_type":"markdown","metadata":{"id":"mIqmojPaWD0H"},"source":["# Very Simple Network"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Ny-mh_ocWIJR","executionInfo":{"status":"ok","timestamp":1649024531713,"user_tz":240,"elapsed":240,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"outputs":[],"source":["class Network(nn.Module):\n","    \"\"\"\n","    The Very Low early deadline architecture is a 4-layer CNN.\n","    The first Conv layer has 64 channels, kernel size 7, and stride 4.\n","    The next three have 128, 256, and 512 channels. Each have kernel size 3 and stride 2.\n","    Think about what the padding should be for each layer to not change spatial resolution.\n","    Each Conv layer is accompanied by a Batchnorm and ReLU layer.\n","    Finally, you want to average pool over the spatial dimensions to reduce them to 1 x 1.\n","    Then, remove (Flatten?) these trivial 1x1 dimensions away.\n","    Look through https://pytorch.org/docs/stable/nn.html \n","    TODO: Fill out the model definition below! \n","\n","    Why does a very simple network have 4 convolutions?\n","    Input images are 224x224. Note that each of these convolutions downsample.\n","    Downsampling 2x effectively doubles the receptive field, increasing the spatial\n","    region each pixel extracts features from. Downsampling 32x is standard\n","    for most image models.\n","\n","    Why does a very simple network have high channel sizes?\n","    Every time you downsample 2x, you do 4x less computation (at same channel size).\n","    To maintain the same level of computation, you 2x increase # of channels, which \n","    increases computation by 4x. So, balances out to same computation.\n","    Another intuition is - as you downsample, you lose spatial information. Want\n","    to preserve some of it in the channel dimension.\n","    \"\"\"\n","    def __init__(self, num_classes=7000):\n","        super().__init__()\n","\n","        self.backbone = nn.Sequential(\n","            # Note that first conv is stride 4. It is (was?) standard to downsample.\n","            # 4x early on, as with 224x224 images, 4x4 patches are just low-level details.\n","            # Food for thought: Why is the first conv kernel size 7, not kernel size 3?\n","\n","            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=4),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2),\n","            nn.BatchNorm2d(128), \n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=2),\n","            nn.BatchNorm2d(512),\n","            nn.AvgPool2d(kernel_size=6),\n","            nn.Flatten()\n","\n","            # TODO: Average pool over & reduce the spatial dimensions to (1, 1)\n","            # TODO: Collapse (Flatten) the trivial (1, 1) dimensions\n","            ) \n","        \n","        self.cls_layer = nn.Linear(512, num_classes)\n","    \n","    def forward(self, x, return_feats=False):\n","        \"\"\"\n","        What is return_feats? It essentially returns the second-to-last-layer\n","        features of a given image. It's a \"feature encoding\" of the input image,\n","        and you can use it for the verification task. You would use the outputs\n","        of the final classification layer for the classification task.\n","\n","        You might also find that the classification outputs are sometimes better\n","        for verification too - try both.\n","        \"\"\"\n","        feats = self.backbone(x)\n","        out = self.cls_layer(feats)\n","\n","        if return_feats:\n","            return feats\n","        else:\n","            return out"]},{"cell_type":"markdown","source":["# MobileNetV2"],"metadata":{"id":"-jhGxUcKstHE"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import math\n","\n","class InvertedResidualBlock(nn.Module):\n","    \n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 stride,\n","                 expand_ratio):\n","        super().__init__() # Just have to do this for all nn.Module classes\n","\n","        # Can only do identity residual connection if input & output are the\n","        # same channel & spatial shape.\n","        if stride == 1 and in_channels == out_channels:\n","            self.do_identity = True\n","        else:\n","            self.do_identity = False\n","\n","        # Expand Ratio is like 6, so hidden_dim >> in_channels\n","        hidden_dim = in_channels * expand_ratio\n","\n","        self.feature_mixing = nn.Sequential(\n","            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(hidden_dim),\n","            nn.ReLU6(),\n","        )\n","\n","        self.spatial_mixing = nn.Sequential(\n","            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, stride=stride, padding=1, groups=hidden_dim, bias=False),\n","            nn.BatchNorm2d(hidden_dim),\n","            nn.ReLU6(),\n","        )\n","        \n","        self.bottleneck_channels = nn.Sequential(\n","            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","        )\n","\n","    def forward(self, x):\n","        out = self.feature_mixing(x)\n","        out = self.spatial_mixing(out)\n","        out = self.bottleneck_channels(out)\n","\n","        if self.do_identity:\n","            return x + out\n","        else:\n","            return out\n","\n","class MobileNetV2(nn.Module):\n","   \n","    def __init__(self, num_classes= 7000):\n","        super().__init__()\n","\n","        self.num_classes = num_classes\n","\n","        self.stem = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU6(),\n","            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, groups=32, bias=False),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU6(),\n","            nn.Conv2d(32, 16, kernel_size=1, stride=1, padding=0, bias=False),\n","            nn.BatchNorm2d(16),\n","        )\n","\n","        self.stage_cfgs = [\n","            # expand_ratio, channels, # blocks, stride of first block\n","            [6,  24, 2, 2],\n","            [6,  32, 3, 2],\n","            [6,  64, 4, 2],\n","            [6,  96, 3, 1],\n","            [6, 160, 3, 2],\n","            [6, 320, 1, 1],\n","        ]\n","\n","        # Remember that our stem left us off at 16 channels. We're going to\n","        # keep updating this in_channels variable as we go\n","        in_channels = 16\n","\n","        # Let's make the layers\n","        layers = []\n","        for curr_stage in self.stage_cfgs:\n","            expand_ratio, num_channels, num_blocks, stride = curr_stage\n","\n","            for block_idx in range(num_blocks):\n","                out_channels = num_channels\n","                layers.append(InvertedResidualBlock(\n","                    in_channels=in_channels,\n","                    out_channels=out_channels,\n","                    # only have non-trivial stride if first block\n","                    stride=stride if block_idx == 0 else 1,\n","                    expand_ratio=expand_ratio\n","                ))\n","                # In channels of the next block is the out_channels of the current one\n","                in_channels = out_channels\n","\n","        self.layers = nn.Sequential(*layers) # Done, save them to the class\n","\n","        # Some final feature mixing\n","        self.final_block = nn.Sequential(\n","            nn.Conv2d(in_channels, 1280, kernel_size=1, padding=0, stride=1, bias=False),\n","            nn.BatchNorm2d(1280),\n","            nn.ReLU6()\n","        )\n","\n","        # Now, we need to build the final classification layer.\n","        self.cls_layer = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.Flatten(),\n","            # nn.Linear(1280, num_classes)\n","        )\n","        self.cls_layer_final = nn.Linear(1280, num_classes)\n","\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        \n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.weight.data.normal_(0, 0.01)\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, return_feats=False):\n","        out = self.stem(x)\n","        out = self.layers(out)\n","        out = self.final_block(out)\n","        feats = self.cls_layer(out)\n","        out = self.cls_layer_final(feats)\n","\n","        if return_feats:\n","            return feats\n","        else:\n","            return out\n"],"metadata":{"id":"3PPkhC8Qsf3-","executionInfo":{"status":"ok","timestamp":1649024533337,"user_tz":240,"elapsed":495,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# ConvNeXt"],"metadata":{"id":"ICQmw6bc2Gjs"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import math\n","\n","class InvertedResidualBlock(nn.Module):\n","    \n","    def __init__(self,\n","                 in_channels,\n","                 out_channels,\n","                 stride,\n","                 expand_ratio):\n","        super().__init__() # Just have to do this for all nn.Module classes\n","\n","        # Can only do identity residual connection if input & output are the\n","        # same channel & spatial shape.\n","        if stride == 1 and in_channels == out_channels:\n","            self.do_identity = True\n","        else:\n","            self.do_identity = False\n","\n","        # Expand Ratio is like 6, so hidden_dim >> in_channels\n","        hidden_dim = in_channels * expand_ratio\n","\n","        self.spatial_mixing = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels, kernel_size=7, stride=stride, padding=3, groups=in_channels, bias=False),\n","            nn.BatchNorm2d(in_channels),\n","            # nn.ReLU6(),\n","        )\n","\n","        self.feature_mixing = nn.Sequential(\n","            nn.Conv2d(in_channels, hidden_dim, kernel_size=1, stride=1, padding=0, bias=False),\n","            # nn.BatchNorm2d(hidden_dim),\n","            nn.GELU(),\n","        )\n","\n","        self.bottleneck_channels = nn.Sequential(\n","            nn.Conv2d(hidden_dim, out_channels, kernel_size=1, stride=1, padding=0, bias=False),\n","            # nn.BatchNorm2d(out_channels),\n","        )\n","\n","    def forward(self, x):\n","        out = self.spatial_mixing(x)\n","        out = self.feature_mixing(out)\n","        out = self.bottleneck_channels(out)\n","\n","        if self.do_identity:\n","            return x + out\n","        else:\n","            return out\n","\n","class ConvNeXt(nn.Module):\n","   \n","    def __init__(self, num_classes= 7000):\n","        super().__init__()\n","\n","        self.num_classes = num_classes\n","\n","        self.stem = nn.Sequential(\n","            nn.Conv2d(3, 96, kernel_size=4, stride=4, padding=0, bias=False),\n","            nn.BatchNorm2d(96),\n","        )\n","\n","        self.stage_cfgs = [\n","            # expand_ratio, channels, # blocks, stride of first block\n","            [4,  96, 3, 1],\n","            [4,  192, 3, 1],\n","            [4,  384, 9, 1],\n","            [4,  768, 3, 1],\n","        ]\n","\n","        self.downsampling_layer = [\n","                                   nn.Sequential(\n","                                       nn.BatchNorm2d(96),\n","                                       nn.Conv2d(96, 192, kernel_size=2, stride=2)),\n","\n","                                   nn.Sequential(\n","                                       nn.BatchNorm2d(192),\n","                                       nn.Conv2d(192, 384, kernel_size=2, stride=2)),\n","\n","                                   nn.Sequential(\n","                                       nn.BatchNorm2d(384),\n","                                       nn.Conv2d(384, 768, kernel_size=2, stride=2))\n","                                   ]\n","\n","        in_channels = 96\n","\n","        # Let's make the layers\n","        layers = []\n","        ix = 0\n","        for curr_stage in self.stage_cfgs:\n","            expand_ratio, num_channels, num_blocks, stride = curr_stage\n","\n","            for block_idx in range(num_blocks):\n","                out_channels = num_channels\n","                layers.append(InvertedResidualBlock(\n","                    in_channels=in_channels,\n","                    out_channels=out_channels,\n","                    # only have non-trivial stride if first block\n","                    stride=stride if block_idx == 0 else 1,\n","                    expand_ratio=expand_ratio\n","                ))\n","                # In channels of the next block is the out_channels of the current one\n","                in_channels = out_channels\n","            if ix < 3:\n","              layers.append(self.downsampling_layer[ix])\n","              ix += 1\n","              in_channels = 2 * in_channels\n","\n","        self.layers = nn.Sequential(*layers) # Done, save them to the class\n","\n","\n","        # Now, we need to build the final classification layer.\n","        self.cls_layer = nn.Sequential(\n","            nn.AdaptiveAvgPool2d((1, 1)),\n","            nn.BatchNorm2d(768),\n","            nn.Flatten(),\n","            nn.Linear(768, num_classes)\n","        )\n","        # self.cls_layer_final = nn.Linear(768, num_classes)\n","        \n","\n","        self._initialize_weights()\n","\n","    def _initialize_weights(self):\n","        \n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.weight.data.normal_(0, 0.01)\n","                m.bias.data.zero_()\n","\n","    def forward(self, x, return_feats=False):\n","        out = self.stem(x)\n","        out = self.layers(out)\n","        out = self.cls_layer(out)\n","        # out = self.cls_layer_final(feats)\n","\n","        if return_feats:\n","            return feats\n","        else:\n","            return out\n"],"metadata":{"id":"SHjKTUBN2LMo","executionInfo":{"status":"ok","timestamp":1649024535170,"user_tz":240,"elapsed":459,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NwYR-CLwX09u"},"source":["# Dataset & DataLoader"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"awE5BxlqX2o7","executionInfo":{"status":"ok","timestamp":1649024540055,"user_tz":240,"elapsed":2742,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"outputs":[],"source":["from torch._C import dtype\n","\"\"\"\n","Transforms (data augmentation) is quite important for this task.\n","Go explore https://pytorch.org/vision/stable/transforms.html for more details\n","\"\"\"\n","DATA_DIR = \"/content\"\n","TRAIN_DIR = osp.join(DATA_DIR, \"classification/classification/train\") # This is a smaller subset of the data. Should change this to classification/classification/train\n","VAL_DIR = osp.join(DATA_DIR, \"classification/classification/dev\")\n","TEST_DIR = osp.join(DATA_DIR, \"classification/classification/test\")\n","\n","train_transforms = [#ttf.ToPILImage(mode='RGB'),\n","                    ttf.RandAugment(), \n","                    ttf.ToTensor(), \n","                    ttf.RandomErasing(p=0.25)\n","                    # ttf.RandomApply(transforms=[ttf.ColorJitter(brightness=.5, hue=.3), \n","                    #                             ttf.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75))\n","                    #                             ], p=0.2)]\n","]\n","\n","                    \n","val_transforms = [ttf.ToTensor()]\n","\n","train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR,\n","                                                 transform=ttf.Compose(train_transforms))\n","val_dataset = torchvision.datasets.ImageFolder(VAL_DIR,\n","                                               transform=ttf.Compose(val_transforms))\n","\n","\n","# Image Augmentation \n","train_augment1 = [ttf.ToTensor(), \n","                  ttf.RandomHorizontalFlip(p=0.4), \n","                  ttf.RandomVerticalFlip(0.4),\n","                  ttf.ColorJitter(brightness=.5, hue=.3)]\n","\n","train_augment2 = [ttf.ToTensor(), \n","                  # ttf.RandomRotation(degrees=(0, 180)), \n","                  # ttf.RandomAffine(degrees=(30, 70), translate=(0.1, 0.3), scale=(0.5, 0.75)),\n","                  ttf.RandomErasing(p=0.3, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0, inplace=False)\n","                  ]\n","\n","train_augmented_dataset1 = torchvision.datasets.ImageFolder(TRAIN_DIR,\n","                                                 transform=ttf.Compose(train_augment1))\n","train_augmented_dataset2 = torchvision.datasets.ImageFolder(TRAIN_DIR,\n","                                                 transform=ttf.Compose(train_augment2))\n","\n","# Concatenate two datasets\n","image_datasets = torch.utils.data.ConcatDataset([train_dataset, train_augmented_dataset2])\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size,\n","                          shuffle=True, drop_last=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n","                        drop_last=True, num_workers=1)"]},{"cell_type":"markdown","source":["# Triple Loss"],"metadata":{"id":"oQ9l5WdDhpri"}},{"cell_type":"code","source":["class TripletDataset(torchvision.datasets.VisionDataset):\n","  def __init__(self, root, transform):  \n","    # For \"root\", note that you're making this dataset on top of the regular classification dataset.\n","    self.dataset = torchvision.datasets.ImageFolder(root=root, transform=transform)\n","    \n","    # map class indices to dataset image indices\n","    self.classes_to_img_indices = [[] for _ in range(len(self.dataset.classes))]\n","    for img_idx, (_, class_id) in enumerate(self.dataset.samples):\n","      self.classes_to_img_indices[class_id].append(img_idx)\n","    \n","    # VisionDataset attributes for display\n","    self.root = root\n","    self.length = len(self.dataset.classes) # pseudo length! Length of this dataset is 7000, *not* the actual # of images in the dataset. You can just increase the # of epochs you train for.\n","    self.transforms = self.dataset.transforms\n","          \n","  def __len__(self):\n","    return self.length\n","    \n","  def __getitem__(self, anchor_class_idx):\n","    \"\"\"Treat the given index as the anchor class and pick a triplet randomly\"\"\"\n","    anchor_class = self.classes_to_img_indices[anchor_class_idx]\n","    # choose positive pair (assuming each class has at least 2 images)\n","    anchor, positive = np.random.choice(a=anchor_class, size=2, replace=False)\n","    # choose negative image\n","    # hint for further exploration: you can choose 2 negative images to make it a Quadruplet Loss\n","\n","    classes_to_choose_negative_class_from = list(range(self.length))\n","    classes_to_choose_negative_class_from.pop(anchor_class_idx) # TODO: What are we removing?\n","    negative_class_idx = random.choice(classes_to_choose_negative_class_from)\n","    negative_class = self.classes_to_img_indices[negative_class_idx]\n","    negative = np.random.choice(a=negative_class, size=1, replace=False)\n","    \n","    # self.dataset[idx] will return a tuple (image tensor, class label). You can use its outputs to train for classification alongside verification\n","    # If you do not want to train for classification, you can use self.dataset[idx][0] to get the image tensor\n","    return self.dataset[anchor][0], self.dataset[positive][0], self.dataset[int(negative)][0]\n"],"metadata":{"id":"cFef5wDwhnsH","executionInfo":{"status":"ok","timestamp":1649024547164,"user_tz":240,"elapsed":223,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["train_dataset_triple_loss = TripletDataset(TRAIN_DIR, transform=ttf.Compose(train_transforms))"],"metadata":{"id":"iE-E5c8_i2ON","executionInfo":{"status":"ok","timestamp":1649024554758,"user_tz":240,"elapsed":1082,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["train_loader_triple_loss = DataLoader(train_dataset_triple_loss, batch_size=batch_size,\n","                          shuffle=True, drop_last=True, num_workers=2)"],"metadata":{"id":"VEhafX_oi6LA","executionInfo":{"status":"ok","timestamp":1649024556032,"user_tz":240,"elapsed":205,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","def show_img(img):\n","  plt.figure(figsize=(5,3))\n","  npimg=img.numpy()\n","  plt.imshow(np.transpose(npimg,(1,2,0)))\n","  plt.show()\n","\n","def show(imgs):\n","    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n","    for i, img in enumerate(imgs):\n","        img = ttf.ToPILImage()(img.to('cpu'))\n","        axs[0, i].imshow(np.asarray(img))\n","        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])"],"metadata":{"id":"yg6_LphEiUfl","executionInfo":{"status":"ok","timestamp":1649024560583,"user_tz":240,"elapsed":224,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KZCn0qHuZRKj"},"source":["# Setup everything for training"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"UowI9OcUYPjP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649025835422,"user_tz":240,"elapsed":611,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}},"outputId":"cb68dbec-5d5e-454b-b2ed-8c002e8b87f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Params: 11155928\n"]}],"source":["model = MobileNetV2()\n","model.cuda()\n","# model.load_state_dict(torch.load(\"/content/model_face_verif_triple_loss_MobilNetV2_epoch_49.pt\"))\n","\n","\n","# For this homework, we're limiting you to 35 million trainable parameters, as\n","# outputted by this. This is to help constrain your search space and maintain\n","# reasonable training times & expectations\n","num_trainable_parameters = 0\n","for p in model.parameters():\n","    num_trainable_parameters += p.numel()\n","print(\"Number of Params: {}\".format(num_trainable_parameters))\n","\n","# TODO: What criterion do we use for this task?\n","criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n","# criterion = torch.nn.TripletMarginLoss(margin=0.25)\n","optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n","# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=(len(train_loader) * epochs))\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, patience=2, mode='max', threshold=0.01)\n","# T_max is \"how many times will i call scheduler.step() until it reaches 0 lr?\"\n","\n","# For this homework, we strongly strongly recommend using FP16 to speed up training.\n","# It helps more for larger models.\n","# Go to https://effectivemachinelearning.com/PyTorch/8._Faster_training_with_mixed_precision\n","# and compare \"Single precision training\" section with \"Mixed precision training\" section\n","scaler = torch.cuda.amp.GradScaler()"]},{"cell_type":"markdown","metadata":{"id":"dzM11HtcboYv"},"source":["# Let's train!"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"JrChwbscbYkj","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1072f1a-287a-4cfe-da30-8f671346045b","executionInfo":{"status":"ok","timestamp":1649025455321,"user_tz":240,"elapsed":620190,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}}},"outputs":[{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 1/2: Train Acc 0.0136%, Train Loss 8.8644, Learning Rate 0.0005\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Validation: 0.0200%\n"]},{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["Epoch 2/2: Train Acc 0.0143%, Train Loss 8.8465, Learning Rate 0.0000\n"]},{"output_type":"stream","name":"stderr","text":["                                                                   "]},{"output_type":"stream","name":"stdout","text":["Validation: 0.0114%\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["for epoch in range(epochs):\n","    model.train()\n","    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n","    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n","\n","    num_correct = 0\n","    total_loss = 0\n","\n","    for i, (x, y) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","\n","        x = x.cuda()\n","        y = y.cuda()\n","\n","        # Don't be surprised - we just wrap these two lines to make it work for FP16\n","        with torch.cuda.amp.autocast():     \n","            outputs = model(x)\n","            loss = criterion(outputs, y)\n","\n","        # Update # correct & loss as we go\n","        num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n","        total_loss += float(loss)\n","\n","        # tqdm lets you add some details so you can monitor training as you train.\n","        batch_bar.set_postfix(\n","            acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)),\n","            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n","            num_correct=num_correct,\n","            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n","        \n","        # Another couple things you need for FP16. \n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update() # This is something added just for FP16\n","\n","         \n","        scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n","\n","        batch_bar.update() # Update tqdm bar\n","    batch_bar.close() # You need this to close the tqdm bar\n","    torch.save(model.state_dict(), f\"model_ConvNeXt_epoch_{epoch}.pt\")\n","\n","    print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n","        epoch + 1,\n","        epochs,\n","        100 * num_correct / (len(train_loader) * batch_size),\n","        float(total_loss / len(train_loader)),\n","        float(optimizer.param_groups[0]['lr'])))\n","    \n","    # You can add validation per-epoch here if you would like\n","    model.eval()\n","    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n","    num_correct = 0\n","    for i, (x, y) in enumerate(val_loader):\n","\n","      x = x.cuda()\n","      y = y.cuda()\n","\n","      with torch.no_grad():\n","          outputs = model(x)\n","\n","      num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n","      batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n","\n","      batch_bar.update()\n","    \n","    batch_bar.close()\n","    print(\"Validation: {:.04f}%\".format(100 * num_correct / len(val_dataset)))\n","    # scheduler.step((100 * num_correct / len(val_dataset)))"]},{"cell_type":"markdown","source":["# Face verification: Triple Loss Training"],"metadata":{"id":"rE2qM4U5jBQw"}},{"cell_type":"code","source":["for epoch in range(epochs):\n","    model.train()\n","    # Quality of life tip: leave=False and position=0 are needed to make tqdm usable in jupyter\n","    batch_bar = tqdm(total=len(train_loader_triple_loss), dynamic_ncols=True, leave=False, position=0, desc='Train') \n","\n","    # num_correct = 0\n","    # total_loss = 0\n","\n","    for i, (anchor, positive, negative) in enumerate(train_loader_triple_loss):\n","        optimizer.zero_grad()\n","\n","        anchor = anchor.cuda()\n","        positive = positive.cuda()\n","        negative = negative.cuda()\n","\n","        # Don't be surprised - we just wrap these two lines to make it work for FP16\n","        with torch.cuda.amp.autocast():     \n","            outputs_anchor = model(anchor, return_feats=True)\n","            outputs_positive = model(positive, return_feats=True)\n","            outputs_negative = model(negative, return_feats=True)\n","            loss = criterion(outputs_anchor, outputs_positive, outputs_negative)\n","        \n","        # Another couple things you need for FP16. \n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update() # This is something added just for FP16\n","        scheduler.step() # We told scheduler T_max that we'd call step() (len(train_loader) * epochs) many times.\n","\n","        batch_bar.update() # Update tqdm bar\n","    batch_bar.close() # You need this to close the tqdm bar\n","    torch.save(model.state_dict(), f\"model_face_verif_triple_loss_MobilNetV2_epoch_{epoch}.pt\")\n","    print(\"Epoch {}| Learning Rate {:.04f}\".format(\n","        epoch + 1,\n","        epochs,\n","        float(optimizer.param_groups[0]['lr'])))\n","\n","    # scheduler.step((100 * num_correct / len(val_dataset)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ggtoEDvvi_bq","executionInfo":{"status":"ok","timestamp":1649025611555,"user_tz":240,"elapsed":75700,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}},"outputId":"e4031c5a-f6c7-4acf-c79b-192dc9bdcc15"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["Train:   0%|          | 0/109 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1| Learning Rate 2.0000\n"]},{"output_type":"stream","name":"stderr","text":["                                                        "]},{"output_type":"stream","name":"stdout","text":["Epoch 2| Learning Rate 2.0000\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}]},{"cell_type":"markdown","metadata":{"id":"AKb2iD_9gdpX"},"source":["# Classification Task: Validation"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"le1o-OVjfeN9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649026217405,"user_tz":240,"elapsed":69097,"user":{"displayName":"Meiirbek Islamov","userId":"09661373260171358073"}},"outputId":"a2a5ab0a-0639-4e74-a0c1-69dee5d6ca6a"},"outputs":[{"output_type":"stream","name":"stderr","text":["                                                                   "]},{"output_type":"stream","name":"stdout","text":["Validation: 0.0171%\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["# model.load_state_dict(torch.load(\"model_MobilNetV2_epoch_9.pt\"))\n","model.eval()\n","batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n","num_correct = 0\n","for i, (x, y) in enumerate(val_loader):\n","\n","    x = x.cuda()\n","    y = y.cuda()\n","\n","    with torch.no_grad():\n","        outputs = model(x)\n","\n","    num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n","    batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n","\n","    batch_bar.update()\n","    \n","batch_bar.close()\n","print(\"Validation: {:.04f}%\".format(100 * num_correct / len(val_dataset)))"]},{"cell_type":"markdown","metadata":{"id":"UpgCHImRkYQW"},"source":["# Classification Task: Submit to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08Zv2AWFrfVP"},"outputs":[],"source":["class ClassificationTestSet(Dataset):\n","    # It's possible to load test set data using ImageFolder without making a custom class.\n","    # See if you can think it through!\n","\n","    def __init__(self, data_dir, transforms):\n","        self.data_dir = data_dir\n","        self.transforms = transforms\n","\n","        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n","        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","    \n","    def __getitem__(self, idx):\n","        return self.transforms(Image.open(self.img_paths[idx]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"td_qvGwr16z0"},"outputs":[],"source":["test_dataset = ClassificationTestSet(TEST_DIR, ttf.Compose(val_transforms))\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n","                         drop_last=False, num_workers=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2WQEUjXkWvo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647483070032,"user_tz":240,"elapsed":75262,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"outputId":"e37f4cf4-1e48-4d9b-fd57-e2734dabd1b1"},"outputs":[{"output_type":"stream","name":"stderr","text":[""]}],"source":["model.eval()\n","batch_bar = tqdm(total=len(test_loader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n","\n","res = []\n","for i, (x) in enumerate(test_loader):\n","\n","    x = x.cuda()\n","\n","    with torch.no_grad():\n","        outputs = model(x)\n","        pred_y = torch.argmax(outputs, axis=1)\n","        # res.append(outputs)\n","        res.extend(pred_y.tolist())\n","\n","    # num_correct += int((torch.argmax(outputs, axis=1) == y).sum())\n","    # batch_bar.set_postfix(acc=\"{:.04f}%\".format(100 * num_correct / ((i + 1) * batch_size)))\n","    \n","\n","    batch_bar.update()\n","    \n","batch_bar.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vob9a2-HkW_V"},"outputs":[],"source":["with open(\"classification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(test_dataset)):\n","        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", res[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zpxatBfT4jSQ","executionInfo":{"status":"ok","timestamp":1647483089731,"user_tz":240,"elapsed":5414,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"41ae8522-eb7d-4f88-a4c9-9a76917ee4c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n","100% 541k/541k [00:03<00:00, 150kB/s]\n","Successfully submitted to Face Recognition"]}],"source":["!kaggle competitions submit -c 11-785-s22-hw2p2-classification -f classification_early_submission.csv -m \" ConvNeXt submission 35 epochs (randAug, erase)\""]},{"cell_type":"markdown","metadata":{"id":"PsJx1l1T4twC"},"source":["# Verification Task: Validation"]},{"cell_type":"markdown","source":["There are 6K verification dev images, but 166K \"pairs\" for you to compare. So, it's much more efficient to compute the features for the 6K verification images, and just compare afterwards.\n","\n","This will be done by creating a dictionary mapping the image file names to the features. Then, you'll use this dictionary to compute the similarities for each pair."],"metadata":{"id":"FoBFFF8-Lpvj"}},{"cell_type":"code","source":["!ls verification/verification/dev | wc -l\n","!cat verification/verification/verification_dev.csv | wc -l"],"metadata":{"id":"ZV-WsTi9LrVz","executionInfo":{"status":"ok","timestamp":1647452356171,"user_tz":240,"elapsed":698,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5e8fb5f-19d3-4852-bfb7-0c41c4eea522"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6000\n","166801\n"]}]},{"cell_type":"code","source":["class VerificationDataset(Dataset):\n","    def __init__(self, data_dir, transforms):\n","        self.data_dir = data_dir\n","        self.transforms = transforms\n","\n","        # This one-liner basically generates a sorted list of full paths to each image in data_dir\n","        self.img_paths = list(map(lambda fname: osp.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","    \n","    def __getitem__(self, idx):\n","        # We return the image, as well as the path to that image (relative path)\n","        return self.transforms(Image.open(self.img_paths[idx])), osp.relpath(self.img_paths[idx], self.data_dir)"],"metadata":{"id":"m1YtIwxuL7H0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98lmjm0S4tHR"},"outputs":[],"source":["val_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/dev\"),\n","                                       ttf.Compose(val_transforms))\n","val_ver_loader = torch.utils.data.DataLoader(val_veri_dataset, batch_size=batch_size, \n","                                             shuffle=False, num_workers=1)"]},{"cell_type":"code","source":["# model.cuda()\n","model.eval()\n","\n","feats_dict = dict()\n","for batch_idx, (imgs, path_names) in tqdm(enumerate(val_ver_loader), total=len(val_ver_loader), position=0, leave=False):\n","    imgs = imgs.cuda()\n","\n","    with torch.no_grad():\n","        # Note that we return the feats here, not the final outputs\n","        # Feel free to try the final outputs too!\n","        feats = model(imgs, return_feats=True) \n","        # feats = nn.AdaptiveAvgPool2d((1, 1))(feats)\n","        # feats = nn.BatchNorm2d(768)(feats)\n","        # feats = nn.ReLU()(feats)\n","        for i in range(len(path_names)):\n","          feats_dict[path_names[i]] = feats[i].cpu()\n","\n"],"metadata":{"id":"-Qw45H-eMyyn","executionInfo":{"status":"ok","timestamp":1647492117003,"user_tz":240,"elapsed":11937,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4cb7bee9-2024-4569-cdef-7f021050ec9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[""]}]},{"cell_type":"code","source":["from sys import call_tracing\n","call_tracing# What does this dict look like?\n","# print(list(feats_dict.items())[0])"],"metadata":{"id":"k6TG6RD6NTtX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647485514132,"user_tz":240,"elapsed":182,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"outputId":"c0e968cf-ebbf-4921-b138-8325df925ea9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<function sys.call_tracing>"]},"metadata":{},"execution_count":100}]},{"cell_type":"code","source":["# We use cosine similarity between feature embeddings.\n","# TODO: Find the relevant function in pytorch and read its documentation.\n","similarity_metric = nn.CosineSimilarity(dim=0, eps=1e-6)\n","\n","val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_dev.csv\")\n","\n","\n","# Now, loop through the csv and compare each pair, getting the similarity between them\n","pred_similarities = []\n","gt_similarities = []\n","for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n","    img_path1, img_path2, gt = line.split(\",\")\n","\n","    # TODO: Use the similarity metric\n","    # How to use these img_paths? What to do with the features?\n","    similarity = similarity_metric(feats_dict[img_path1.split(\"/\")[1]], feats_dict[img_path2.split(\"/\")[1]])\n","\n","    gt_similarities.append(int(gt))\n","    pred_similarities.append(similarity)\n","\n","pred_similarities = np.array(pred_similarities)\n","gt_similarities = np.array(gt_similarities)\n","\n","print(\"AUC:\", roc_auc_score(gt_similarities, pred_similarities))"],"metadata":{"id":"_zuqds2qNO6N","executionInfo":{"status":"ok","timestamp":1647492192909,"user_tz":240,"elapsed":6737,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"288c0a23-2274-4009-e123-a4a854909c74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[""]},{"output_type":"stream","name":"stdout","text":["AUC: 0.9671810353372368\n"]}]},{"cell_type":"markdown","source":["# Verification Task: Submit to Kaggle"],"metadata":{"id":"sakRa8oZOlKr"}},{"cell_type":"code","source":["test_veri_dataset = VerificationDataset(osp.join(DATA_DIR, \"verification/verification/test\"),\n","                                        ttf.Compose(val_transforms))\n","test_ver_loader = torch.utils.data.DataLoader(test_veri_dataset, batch_size=batch_size, \n","                                              shuffle=False, num_workers=1)"],"metadata":{"id":"oDK3knDcOrOE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","\n","feats_dict = dict()\n","for batch_idx, (imgs, path_names) in tqdm(enumerate(test_ver_loader), total=len(test_ver_loader), position=0, leave=False):\n","    imgs = imgs.cuda()\n","\n","    with torch.no_grad():\n","        # Note that we return the feats here, not the final outputs\n","        # Feel free to try to final outputs too!\n","        feats = model(imgs,return_feats=True) \n","        # feats = nn.AdaptiveAvgPool2d((1, 1))(feats)\n","        # feats = nn.BatchNorm2d(768)(feats)\n","        # feats = nn.GELU()(feats)\n","        for i in range(len(path_names)):\n","          feats_dict[path_names[i]] = feats[i].cpu()\n","    \n","    # TODO: Now we have features and the image path names. What to do with them?\n","    # Hint: use the feats_dict somehow."],"metadata":{"id":"igeRT3WxOrB_","executionInfo":{"status":"ok","timestamp":1647492368589,"user_tz":240,"elapsed":44179,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d193bccd-b78a-4ab0-e68b-bec4a6cc2da6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[""]}]},{"cell_type":"code","source":["# We use cosine similarity between feature embeddings.\n","# TODO: Find the relevant function in pytorch and read its documentation.\n","similarity_metric = nn.CosineSimilarity(dim=0, eps=1e-6)\n","val_veri_csv = osp.join(DATA_DIR, \"verification/verification/verification_test.csv\")\n","\n","\n","# Now, loop through the csv and compare each pair, getting the similarity between them\n","pred_similarities = []\n","for line in tqdm(open(val_veri_csv).read().splitlines()[1:], position=0, leave=False): # skip header\n","    img_path1, img_path2 = line.split(\",\")\n","\n","    # TODO: Finish up verification testing.\n","    # How to use these img_paths? What to do with the features?\n","    similarity = similarity_metric(feats_dict[img_path1.split(\"/\")[1]], feats_dict[img_path2.split(\"/\")[1]])\n","    pred_similarities.append(similarity)"],"metadata":{"id":"X4OZL_FNOq1r","executionInfo":{"status":"ok","timestamp":1647492412894,"user_tz":240,"elapsed":16047,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"05798abe-cae5-4172-b6f8-cedb3ef1ff86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[""]}]},{"cell_type":"code","source":["with open(\"verification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,match\\n\")\n","    for i in range(len(pred_similarities)):\n","        f.write(\"{},{}\\n\".format(i, pred_similarities[i]))"],"metadata":{"id":"fYXiglWkPBDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5zB7P8O687N","executionInfo":{"status":"ok","timestamp":1647492453842,"user_tz":240,"elapsed":6876,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"16a5d9ae-b537-4f6c-8d7d-80a482761c3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n","100% 16.4M/16.4M [00:04<00:00, 3.50MB/s]\n","Successfully submitted to Face Verification"]}],"source":["!kaggle competitions submit -c 11-785-s22-hw2p2-verification -f verification_early_submission.csv -m \"MobileNetv2 (model_epoch_26) triple loss 150 epochs\""]},{"cell_type":"markdown","metadata":{"id":"ALiq9PTl7KwY"},"source":["# Extras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuAsK_tKhzH9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647485602006,"user_tz":240,"elapsed":355,"user":{"displayName":"Meiirbek Islamov","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"09661373260171358073"}},"outputId":"be2ff53b-aa45-4ef5-96e0-11cbf29ce835"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Mar 17 02:53:21 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    33W / 250W |  16209MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# If you keep re-initializing your model in Colab, can run out of GPU memory, need to restart.\n","# These three lines can help that - run this before you re-initialize your model\n","\n","del model\n","torch.cuda.empty_cache()\n","!nvidia-smi"]},{"cell_type":"code","source":[""],"metadata":{"id":"--ptd-RSM9OS"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["mIqmojPaWD0H","-jhGxUcKstHE","ICQmw6bc2Gjs"],"name":"Clean_HW2P2_Starter.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}